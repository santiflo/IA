{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller 3: Aprendizaje profundo sobre el censo de 1994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Santiago Florian Bustamante_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "El siguiente taller es con el fin de evaluar el desempeño de aprendizaje profundo visto en clase sobre un mismo conjunto de datos evaluado en el taller 2. Esto es con el fin de comparar el desempeño producido por el metodo de aprendizaje profundo en contraste con las tecnicas evaluadas.\n",
    "\n",
    "\n",
    "## Definición de la tarea\n",
    "El conjunto de datos en el que se va a trabajara es una base de datos de 1994 brindada por UCI Maching Learning Repository abierta al público desde 1996 llamada \"adult”. La cual cuenta con la cantidad de 48842 instancias divididas en 14 atributos con valores mixtos; continuos y discretos. La tarea de clasificación es determinar si una persona gana menos de 50k o más de 50k.\n",
    "\n",
    "los 14 atributos son los siguientes:\n",
    "* age(0): continuous.\n",
    "\n",
    "* workclass(1): Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "\n",
    "* fnlwgt(2): continuous.\n",
    "\n",
    "* education(3): Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "\n",
    "* education-num(4): continuous.\n",
    "\n",
    "* marital-status(5): Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "\n",
    "* occupation(6): Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "\n",
    "* relationship(7): Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "\n",
    "* race(8): White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "\n",
    "* sex(9): Female, Male.\n",
    "\n",
    "* capital-gain(10): continuous.\n",
    "\n",
    "* capital-loss(11): continuous.\n",
    "\n",
    "* hours-per-week(12): continuous.\n",
    "\n",
    "* native-country(13): United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "El 15 valor en los datos equivale a  >50K, <=50K, que respectivamente indica cuando una persona gana 50k más o cuando gana menos de 50k.\n",
    "\n",
    "## Preparar la información\n",
    "### Cargar la información\n",
    "\n",
    "Haciendo uso de la librería pandas, se pueden cargar los archivos \"adult.data\" y \"adult.test\" los cuales contienen todos los datos usados para el desarrollo del taller. El documento \"adult.data\" contiene 32561 instancias y el documento \"adult.test\" contiene 16281 instancias.\n",
    "\n",
    "\"Siempre que trabajemos con algoritmos de machine learning que utilicen un proceso estocástico (por ejemplo, números aleatorios), es una buena idea establecer la semilla del número aleatorio.\" Esto es para que pueda ejecutar el mismo código una y otra vez y obtener el mismo resultado. Esto es útil si necesita demostrar un resultado, comparar algoritmos usando la misma fuente de aleatoriedad o depurar una parte de su código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15) (16281, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32556</td>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32557</td>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32558</td>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32559</td>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32560</td>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                  1       2            3   4                    5   \\\n",
       "0      39          State-gov   77516    Bachelors  13        Never-married   \n",
       "1      50   Self-emp-not-inc   83311    Bachelors  13   Married-civ-spouse   \n",
       "2      38            Private  215646      HS-grad   9             Divorced   \n",
       "3      53            Private  234721         11th   7   Married-civ-spouse   \n",
       "4      28            Private  338409    Bachelors  13   Married-civ-spouse   \n",
       "...    ..                ...     ...          ...  ..                  ...   \n",
       "32556  27            Private  257302   Assoc-acdm  12   Married-civ-spouse   \n",
       "32557  40            Private  154374      HS-grad   9   Married-civ-spouse   \n",
       "32558  58            Private  151910      HS-grad   9              Widowed   \n",
       "32559  22            Private  201490      HS-grad   9        Never-married   \n",
       "32560  52       Self-emp-inc  287927      HS-grad   9   Married-civ-spouse   \n",
       "\n",
       "                       6               7       8        9      10  11  12  \\\n",
       "0            Adm-clerical   Not-in-family   White     Male   2174   0  40   \n",
       "1         Exec-managerial         Husband   White     Male      0   0  13   \n",
       "2       Handlers-cleaners   Not-in-family   White     Male      0   0  40   \n",
       "3       Handlers-cleaners         Husband   Black     Male      0   0  40   \n",
       "4          Prof-specialty            Wife   Black   Female      0   0  40   \n",
       "...                   ...             ...     ...      ...    ...  ..  ..   \n",
       "32556        Tech-support            Wife   White   Female      0   0  38   \n",
       "32557   Machine-op-inspct         Husband   White     Male      0   0  40   \n",
       "32558        Adm-clerical       Unmarried   White   Female      0   0  40   \n",
       "32559        Adm-clerical       Own-child   White     Male      0   0  20   \n",
       "32560     Exec-managerial            Wife   White   Female  15024   0  40   \n",
       "\n",
       "                   13      14  \n",
       "0       United-States   <=50K  \n",
       "1       United-States   <=50K  \n",
       "2       United-States   <=50K  \n",
       "3       United-States   <=50K  \n",
       "4                Cuba   <=50K  \n",
       "...               ...     ...  \n",
       "32556   United-States   <=50K  \n",
       "32557   United-States    >50K  \n",
       "32558   United-States   <=50K  \n",
       "32559   United-States   <=50K  \n",
       "32560   United-States    >50K  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Pandas es una libreria que ofrece estructuras de datos y operaciones para manipular tablas numéricas y series temporales.\n",
    "import pandas as pd\n",
    "#LabelEncoder: Libreria que permite dividir el conjunto de datos en subconjuntos\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#train_test_split: Libreria que permite dividir el conjunto de datos en subconjuntos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Crea MLP en Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "# Fija las semillas aleatorias para la reproducibilidad\n",
    "np.random.seed(7)\n",
    "\n",
    "#Leer la informacion del archivo de datos para entrenamiento\n",
    "train = pd.read_csv(\"adult.data\",header=None, na_values=[' ?'])\n",
    "test = pd.read_csv(\"adult.test\",header=None, na_values=[' ?'])\n",
    "\n",
    "#*.shape permite saber la dimension del objeto\n",
    "print(train.shape,test.shape)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de la información\n",
    "La base de datos \"adult\", es una base de datos bastante completa lo cual la hace perfecta para practicar las técnicas de aprendizaje, aun así, algunas instancias serán eliminadas debido a que no presentan valores en algunos de sus atributos. Lo cual deja al conjunto de entrenamiento con un total de 30162 instancias y para el conjunto de testing 15060.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30162, 15) (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "#*.dropna(inplace=True): Mantenga el DataFrame con entradas válidas en la misma variable.\n",
    "train.dropna(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de haber eliminado los datos faltantes es necesario transofrmar los valores categoricos a valores numericos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelEncoder: transforma los atributos categoricos a numericos\n",
    "train[1]= LabelEncoder().fit_transform(train[1])   #workclass\n",
    "train[3]= LabelEncoder().fit_transform(train[3])   #education\n",
    "train[5]= LabelEncoder().fit_transform(train[5])   #marital-status\n",
    "train[6]= LabelEncoder().fit_transform(train[6])   #occupation\n",
    "train[7]= LabelEncoder().fit_transform(train[7])   #relationship\n",
    "train[8]= LabelEncoder().fit_transform(train[8])   #race\n",
    "train[9]= LabelEncoder().fit_transform(train[9])   #sex\n",
    "train[13]= LabelEncoder().fit_transform(train[13]) #native-contry\n",
    "train[14]= LabelEncoder().fit_transform(train[14]) #el dinero que gana actualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test[1]= LabelEncoder().fit_transform(test[1])    #workclass\n",
    "test[3]= LabelEncoder().fit_transform(test[3])    #education\n",
    "test[5]= LabelEncoder().fit_transform(test[5])    #marital-status\n",
    "test[6]= LabelEncoder().fit_transform(test[6])    #occupation\n",
    "test[7]= LabelEncoder().fit_transform(test[7])    #relationship\n",
    "test[8]= LabelEncoder().fit_transform(test[8])    #race\n",
    "test[9]= LabelEncoder().fit_transform(test[9])    #sex\n",
    "test[13]= LabelEncoder().fit_transform(test[13])  #native-contry\n",
    "test[14]= LabelEncoder().fit_transform(test[14])  #el dinero que gana actualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se va identificar los datos atípicos de los atributos continuos. Los datos atípicos que sean encontrados su valor serán remplazados por la media del atributo.\n",
    "\n",
    "La media equivale u una medida de tendencia central y desviacion estandar a una medidad de dispercion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         38.437902\n",
      "1          2.199324\n",
      "2     189793.833930\n",
      "3         10.333764\n",
      "4         10.121312\n",
      "5          2.580134\n",
      "6          5.959850\n",
      "7          1.418341\n",
      "8          3.678602\n",
      "9          0.675685\n",
      "10      1092.007858\n",
      "11        88.372489\n",
      "12        40.931238\n",
      "13        36.382567\n",
      "14         0.248922\n",
      "dtype: float64\n",
      "0         13.134665\n",
      "1          0.953925\n",
      "2     105652.971529\n",
      "3          3.812292\n",
      "4          2.549995\n",
      "5          1.498016\n",
      "6          4.029566\n",
      "7          1.601338\n",
      "8          0.834709\n",
      "9          0.468126\n",
      "10      7406.346497\n",
      "11       404.298370\n",
      "12        11.979984\n",
      "13         6.105372\n",
      "14         0.432396\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "media=train.mean() #Media de cada atributo\n",
    "std= train.std() # desviación de cada atributo\n",
    "print(media)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos atípicos serán identificados sobre 3 desviaciones de la media. Se encontraron 2411 datos atípicos al utilizar 3 desviaciones estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = [0,2,4,10,11,12] #Atributos continuos: age, fnlwgt, education-num, capital-gain, capital-loss, hours-per-week\n",
    "cnt = 0\n",
    "for i in att:\n",
    "    for j in range(len(train[i])):\n",
    "        try:\n",
    "            if(train[i][j]>media[i]+3*std[i]):\n",
    "                train[i][j]=media[i]\n",
    "                cnt = cnt+1\n",
    "            elif(train[i][j]<media[i]-3*std[i]):\n",
    "                train[i][j]=media[i]\n",
    "                cnt =cnt+1\n",
    "        except: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Coeficiente de Correlación es un valor cuantitativo de la relación entre dos o más variables. El coeficiente de correlación puede variar desde -1.00 hasta 1.00. La correlación de proporcionalidad directa o positiva se establece con los valores +1.00 y de proporcionalidad inversa o negativa, con -1.00. No existe relación entre las variables cuando el coeficiente es de 0.00.\n",
    "\n",
    "Al aplicar el coeficiente de coeficiente de correlación en los datos se puede identificar que no existe correlación alguna entre los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>0.073748</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.056588</td>\n",
       "      <td>0.290154</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.248811</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.105178</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.120187</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.248910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.037233</td>\n",
       "      <td>0.034241</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>0.067417</td>\n",
       "      <td>0.044731</td>\n",
       "      <td>0.074973</td>\n",
       "      <td>0.016168</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.038067</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.018044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.073748</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.031942</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.025967</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.018784</td>\n",
       "      <td>0.066991</td>\n",
       "      <td>0.009357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.017855</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332256</td>\n",
       "      <td>0.040664</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.063019</td>\n",
       "      <td>0.078790</td>\n",
       "      <td>0.078987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056588</td>\n",
       "      <td>0.037233</td>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.332256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058239</td>\n",
       "      <td>0.090053</td>\n",
       "      <td>0.096566</td>\n",
       "      <td>0.031790</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.127463</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.162857</td>\n",
       "      <td>0.058860</td>\n",
       "      <td>0.336678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.290154</td>\n",
       "      <td>0.034241</td>\n",
       "      <td>0.031942</td>\n",
       "      <td>0.040664</td>\n",
       "      <td>0.058239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>0.177964</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.119813</td>\n",
       "      <td>0.052686</td>\n",
       "      <td>0.025287</td>\n",
       "      <td>0.195004</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>0.193518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.015572</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.090053</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.062313</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.051577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.248811</td>\n",
       "      <td>0.067417</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.096566</td>\n",
       "      <td>0.177964</td>\n",
       "      <td>0.053727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>0.584876</td>\n",
       "      <td>0.074709</td>\n",
       "      <td>0.023259</td>\n",
       "      <td>0.266944</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>0.251003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.044731</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.011154</td>\n",
       "      <td>0.031790</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089186</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>0.052003</td>\n",
       "      <td>0.124514</td>\n",
       "      <td>0.071658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.074973</td>\n",
       "      <td>0.025967</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.119813</td>\n",
       "      <td>0.062313</td>\n",
       "      <td>0.584876</td>\n",
       "      <td>0.089186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>0.235553</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.216699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.105178</td>\n",
       "      <td>0.016168</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.127463</td>\n",
       "      <td>0.052686</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.074709</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.275481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.032850</td>\n",
       "      <td>0.025287</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.023259</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025248</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.066608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.120187</td>\n",
       "      <td>0.038067</td>\n",
       "      <td>0.018784</td>\n",
       "      <td>0.063019</td>\n",
       "      <td>0.162857</td>\n",
       "      <td>0.195004</td>\n",
       "      <td>0.009222</td>\n",
       "      <td>0.266944</td>\n",
       "      <td>0.052003</td>\n",
       "      <td>0.235553</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.025248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.241097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.066991</td>\n",
       "      <td>0.078790</td>\n",
       "      <td>0.058860</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>0.124514</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.011003</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.248910</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>0.009357</td>\n",
       "      <td>0.078987</td>\n",
       "      <td>0.336678</td>\n",
       "      <td>0.193518</td>\n",
       "      <td>0.051577</td>\n",
       "      <td>0.251003</td>\n",
       "      <td>0.071658</td>\n",
       "      <td>0.216699</td>\n",
       "      <td>0.275481</td>\n",
       "      <td>0.066608</td>\n",
       "      <td>0.241097</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.078883  0.073748  0.001921  0.056588  0.290154  0.005466   \n",
       "1   0.078883  1.000000  0.029983  0.017855  0.037233  0.034241  0.015572   \n",
       "2   0.073748  0.029983  1.000000  0.026697  0.035581  0.031942  0.002003   \n",
       "3   0.001921  0.017855  0.026697  1.000000  0.332256  0.040664  0.038212   \n",
       "4   0.056588  0.037233  0.035581  0.332256  1.000000  0.058239  0.090053   \n",
       "5   0.290154  0.034241  0.031942  0.040664  0.058239  1.000000  0.022655   \n",
       "6   0.005466  0.015572  0.002003  0.038212  0.090053  0.022655  1.000000   \n",
       "7   0.248811  0.067417  0.008034  0.012717  0.096566  0.177964  0.053727   \n",
       "8   0.023726  0.044731  0.013508  0.011154  0.031790  0.068627  0.000717   \n",
       "9   0.083199  0.074973  0.025967  0.027888  0.007680  0.119813  0.062313   \n",
       "10  0.105178  0.016168  0.004213  0.024792  0.127463  0.052686  0.008472   \n",
       "11  0.034169  0.001168  0.005314  0.009558  0.032850  0.025287  0.001774   \n",
       "12  0.120187  0.038067  0.018784  0.063019  0.162857  0.195004  0.009222   \n",
       "13  0.001237  0.007668  0.066991  0.078790  0.058860  0.025902  0.003483   \n",
       "14  0.248910  0.018044  0.009357  0.078987  0.336678  0.193518  0.051577   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.248811  0.023726  0.083199  0.105178  0.034169  0.120187  0.001237   \n",
       "1   0.067417  0.044731  0.074973  0.016168  0.001168  0.038067  0.007668   \n",
       "2   0.008034  0.013508  0.025967  0.004213  0.005314  0.018784  0.066991   \n",
       "3   0.012717  0.011154  0.027888  0.024792  0.009558  0.063019  0.078790   \n",
       "4   0.096566  0.031790  0.007680  0.127463  0.032850  0.162857  0.058860   \n",
       "5   0.177964  0.068627  0.119813  0.052686  0.025287  0.195004  0.025902   \n",
       "6   0.053727  0.000717  0.062313  0.008472  0.001774  0.009222  0.003483   \n",
       "7   1.000000  0.117143  0.584876  0.074709  0.023259  0.266944  0.010809   \n",
       "8   0.117143  1.000000  0.089186  0.017989  0.008109  0.052003  0.124514   \n",
       "9   0.584876  0.089186  1.000000  0.062121  0.007943  0.235553  0.000618   \n",
       "10  0.074709  0.017989  0.062121  1.000000  0.020478  0.083770  0.011003   \n",
       "11  0.023259  0.008109  0.007943  0.020478  1.000000  0.025248  0.001223   \n",
       "12  0.266944  0.052003  0.235553  0.083770  0.025248  1.000000  0.006109   \n",
       "13  0.010809  0.124514  0.000618  0.011003  0.001223  0.006109  1.000000   \n",
       "14  0.251003  0.071658  0.216699  0.275481  0.066608  0.241097  0.023268   \n",
       "\n",
       "          14  \n",
       "0   0.248910  \n",
       "1   0.018044  \n",
       "2   0.009357  \n",
       "3   0.078987  \n",
       "4   0.336678  \n",
       "5   0.193518  \n",
       "6   0.051577  \n",
       "7   0.251003  \n",
       "8   0.071658  \n",
       "9   0.216699  \n",
       "10  0.275481  \n",
       "11  0.066608  \n",
       "12  0.241097  \n",
       "13  0.023268  \n",
       "14  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCorr = train.corr()\n",
    "absTrainCorr = np.absolute(trainCorr)\n",
    "absTrainCorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que existe un desbalanceo ya que el conjunto 0 tiene el 75% de los datos, mientras que el conjunto 1 tiene el 25% restante de los datos. Esto puede generar un mejor aprendizaje para clasificar de manera correcta los datos pertenecientes al conjunto 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22654\n",
      "1     7508\n",
      "Name: 14, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.value_counts(train[14], sort = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de la información a conjuntos\n",
    "Ya que la base de datos ha sido adaptada para realizar el aprendizaje, se pueden dividir los conjuntos para entrenamiento, validación y testing. Debido a que la base de datos ya se encuentra presenta el conjunto de entrenamiento y testing, solo vamos a generar el conjunto de validación, el cual equivale al 10% de las instancias del conjunto de entrenamiento.\n",
    "Dejando de esta manera los conjuntos:\n",
    "* entrenamiento: 27145 instancias\n",
    "* validación: 3017 instancias\n",
    "* testing: 15060 instancias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento por validar:  (27145, 15)\n",
      "Conjunto de validacion:  (3017, 15)\n",
      "Conjunto de entrenamiento:  (30162, 15)\n",
      "Conjunto de testing:  (15060, 15)\n"
     ]
    }
   ],
   "source": [
    "train_t,validation = train_test_split(train, test_size=0.10)\n",
    "print(\"Conjunto de entrenamiento por validar: \",train_t.shape)\n",
    "print(\"Conjunto de validacion: \",validation.shape)\n",
    "print(\"Conjunto de entrenamiento: \", train.shape)\n",
    "print(\"Conjunto de testing: \", test.shape)\n",
    "\n",
    "#Separacion de los atributos de la clasificacion\n",
    "#Atributos training\n",
    "x_t = train_t.loc[:,:13]\n",
    "#Clasificacion training\n",
    "y_t = train_t.loc[:,14]\n",
    "\n",
    "#Atributos de validacion\n",
    "val_x = validation.loc[:,:13]\n",
    "#Clasificacion de validacion\n",
    "val_y = validation.loc[:,14]\n",
    "\n",
    "#Atributos de entrenamiento completo\n",
    "x = train.loc[:,:13]\n",
    "#Clasificacion de entrenamiento completo\n",
    "y = train.loc[:,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnica de aprendizaje profundo - Redes neuronales\n",
    "Es un método de aprendizaje automático el cual se asemeja a una red neuronal. Donde el perceptrón es la neurona y, los axones y dendritas son las conexiones que reciben y envían información a otros perceptrones.\n",
    "## Validación de parámetros\n",
    "Antes de generar un agente inteligente es necesario estimar algunos parámetros para obtener el perceptrón que mejor clasifique los datos.\n",
    "\n",
    "### Función de activación\n",
    "Los perceptrones tienen en cada entrada y salida una \"función de activación\", la cual permite generar umbral donde transforma la salida a 1 o 0 si el umbral es superado o no. Dicha función de activación se puede calcular de 3 maneras:\n",
    "* Función logística, también llamada función sigmoide\n",
    "* Función tangente hiperbólica, también llamada Tanh\n",
    "* Función de activación rectificadora proporciona mejores resultados.\n",
    "\n",
    "### Redes de Neuronas\n",
    "Las neuronas están dispuestas en redes de neuronas. Una fila de neuronas se llama una capa y una red puede tener múltiples capas. La arquitectura de las neuronas en la red se llama a menudo la topología de la red.\n",
    "\n",
    "### Entrada o capas visibles\n",
    "se llama la capa visible, porque es la parte expuesta de la red. A menudo, una red neuronal se dibuja con una capa visible con una por valor de entrada o columna en su conjunto de datos. Estas no son neuronas como se describió anteriormente, sino que simplemente pasa el valor de entrada a la siguiente capa.\n",
    "\n",
    "Los modelos en Keras se definen como una secuencia de capas. Creamos un modelo secuencial y añadimos capas una a una hasta que estamos satisfechos con nuestra topología de red. Lo primero que hay que hacer es asegurarse de que la capa de entrada tiene el número correcto de entradas. Esto se puede especificar al crear la primera capa con el argumento input_dim y establecerlo en 14 para las 14 variables de entrada.\n",
    "\n",
    "### Capas ocultas\n",
    "se llaman capas ocultas porque no están expuestas directamente a la entrada.  El Deep Learning se refiere a tener muchas capas ocultas en tu red neural.\n",
    "\n",
    "### Capa de salida\n",
    "capa de salida y es responsable de emitir un valor o vector de valores que corresponden al formato requerido para el problema. La elección de la función de activación en la capa de salida está fuertemente limitada por el tipo de problema que usted está modelando.\n",
    "\n",
    "Las capas completamente conectadas se densifican utilizando la clase Dense. Podemos especificar el número de neuronas en la capa como el primer argumento y especificar la función de activación usando el argumento de activación. Usaremos la función de activación del rectificador \"relu\" en las primeras capas y la función de activación sigmoide en la capa de salida. Antes las funciones de activación sigmoide y tanh eran preferidas para todas las capas. Hoy en día, se observa un mejor rendimiento utilizando la función de activación del rectificador. Utilizamos una función de activación sigmoide en la capa de salida para asegurarnos de que nuestra salida de red está entre 0 y 1 y es fácil de mapear a cualquier probabilidad de clase 1 o encajar en una clase dura de cualquiera de las dos clases con un umbral por defecto de 0,5. Unimos todo sumando cada capa.\n",
    "\n",
    "### Descenso estocástico por gradiente\n",
    "El clásico y aún preferido algoritmo de entrenamiento para redes neuronales se llama el Descenso de Gradientes Estocástico. Aquí es donde una fila de datos se expone a la red como entrada. La red procesa la entrada hacia delante activando las neuronas hasta que finalmente se produce un valor de salida. Esto se llama un pase adelante en la red. Es el tipo de pase que también es usado después de que la red es entrenada para hacer predicciones sobre nuevos datos. La salida del grafo se compara con la salida esperada y se calcula un error. Este error es entonces propagado a través de la red, en todas las capas y los pesos se actualizan en función del importe que han contribuido al error. Esta inteligente parte de las matemáticas se llama el algoritmo de retro propagación. El proceso se repite para todos los ejemplos en sus datos de formación. Una ronda de actualización de la red para todo el conjunto de datos de formación se denomina época o epoch. Una red puede ser entrenada por decenas, cientos o muchos miles de épocas.\n",
    "\n",
    "### Actualizaciones de peso\n",
    "Los pesos en la red se pueden actualizar a partir de los errores calculados para cada ejemplo de formación y esto se llama aprendizaje en línea. Puede resultar en cambios rápidos, pero también caóticos en la red.\n",
    "Alternativamente, los errores se pueden guardar en todos los ejemplos de formación y la red puede actualizarse al final. Esto se llama aprendizaje por lotes y a menudo es más estable. Debido a que los conjuntos de datos son tan grandes y debido a las eficiencias computacionales, el tamaño del lote, el número de ejemplos que muestra la red antes de una actualización a menudo se reduce a un número pequeño, como decenas o cientos de ejemplos. El importe que se actualizan las ponderaciones es controlado por un parámetro de congestión llamado velocidad de aprendizaje. También se le llama el tamaño del paso y controla el paso o cambio hecho a los pesos de red para un error dado. La ecuación de actualización se puede complementar con términos de congestión adicionales que usted puede establecer.\n",
    "\n",
    "Debemos especificar la función de pérdida a utilizar para evaluar un conjunto de pesos, el optimizador utilizado para buscar a través de diferentes pesos para la red y cualquier métrica opcional que nos gustaría recopilar y reportar durante el entrenamiento.\n",
    "\n",
    "En este caso, utilizaremos la pérdida logarítmica, que para un problema de clasificación binaria se define en Keras como “binary_crossentropy”. También utilizaremos el algoritmo de descenso de gradiente eficiente “adam” por su alta eficiencia en estos problemas.\n",
    "\n",
    "Finalmente, debido a que es un problema de clasificación, recopilaremos y reportaremos el reporte de la clasificacion, el accuracy y la matriz de confusion.\n",
    "\n",
    "### Modelo de ajuste de la Red Neuronal\n",
    "El proceso de entrenamiento se ejecutará para un número fijo de iteraciones denominado epochs o épocas. También podemos establecer el número de instancias que se evalúan antes de que se realice una actualización de peso en la red llamada batch_size y establecerlo mediante el argumento batch_size. Para este problema utilizaremos un pequeño número de epochs (5, 10, 15) y un batch_size relativamente pequeño (89). El batch_size se calculó encontrando los divisores de 27145, el número de instancias del conjunto de entrenamiento sin validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 1 capa\n",
      "\n",
      "N° de epocas: 5\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      2512\n",
      "           1       0.30      0.46      0.36       505\n",
      "\n",
      "    accuracy                           0.73      3017\n",
      "   macro avg       0.59      0.62      0.60      3017\n",
      "weighted avg       0.78      0.73      0.75      3017\n",
      "\n",
      "Accuracy score:  0.731852833941001\n",
      "Confusion matrix:\n",
      "\n",
      "[[1978  275]\n",
      " [ 534  230]]\n",
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86      2768\n",
      "           1       0.20      0.61      0.30       249\n",
      "\n",
      "    accuracy                           0.76      3017\n",
      "   macro avg       0.58      0.69      0.58      3017\n",
      "weighted avg       0.89      0.76      0.81      3017\n",
      "\n",
      "Accuracy score:  0.7643354325488896\n",
      "Confusion matrix:\n",
      "\n",
      "[[2155   98]\n",
      " [ 613  151]]\n",
      "N° de epocas: 15\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2908\n",
      "           1       0.12      0.85      0.21       109\n",
      "\n",
      "    accuracy                           0.77      3017\n",
      "   macro avg       0.56      0.81      0.54      3017\n",
      "weighted avg       0.96      0.77      0.84      3017\n",
      "\n",
      "Accuracy score:  0.772290354656944\n",
      "Confusion matrix:\n",
      "\n",
      "[[2237   16]\n",
      " [ 671   93]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo de 1 capa\\n\")\n",
    "for i in range(5,16,5):\n",
    "    #Atributos training          -> x_t\n",
    "    #Clasificacion training      -> y_t\n",
    "    #Atributos de validacion     -> val_x\n",
    "    #Clasificacion de validacion -> val_y\n",
    "    model = Sequential()\n",
    "    #Modelo con 1 capa\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compila el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # Ajusta el modelo\n",
    "    model.fit(x_t, y_t, epochs=i, batch_size=89, verbose=0)\n",
    "    pred = model.predict_classes(val_x)\n",
    "    \n",
    "    #Resultados del clasificador\n",
    "    print(\"N° de epocas: \"+str(i)+\"\\n\")\n",
    "    print(classification_report(pred,val_y))\n",
    "    print(\"Accuracy score: \",accuracy_score(val_y,pred))\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    print(confusion_matrix(val_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 2 capas\n",
      "\n",
      "N° de epocas: 5\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.25      0.40      3017\n",
      "\n",
      "    accuracy                           0.25      3017\n",
      "   macro avg       0.50      0.13      0.20      3017\n",
      "weighted avg       1.00      0.25      0.40      3017\n",
      "\n",
      "Accuracy score:  0.2532316871063971\n",
      "Confusion matrix:\n",
      "\n",
      "[[   0 2253]\n",
      " [   0  764]]\n",
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2894\n",
      "           1       0.13      0.84      0.23       123\n",
      "\n",
      "    accuracy                           0.77      3017\n",
      "   macro avg       0.56      0.80      0.55      3017\n",
      "weighted avg       0.96      0.77      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7742790851839576\n",
      "Confusion matrix:\n",
      "\n",
      "[[2233   20]\n",
      " [ 661  103]]\n",
      "N° de epocas: 15\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86      2779\n",
      "           1       0.20      0.63      0.30       238\n",
      "\n",
      "    accuracy                           0.77      3017\n",
      "   macro avg       0.58      0.71      0.58      3017\n",
      "weighted avg       0.90      0.77      0.82      3017\n",
      "\n",
      "Accuracy score:  0.7679814385150812\n",
      "Confusion matrix:\n",
      "\n",
      "[[2166   87]\n",
      " [ 613  151]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo de 2 capas\\n\")\n",
    "for i in range(5,16,5):\n",
    "    #Atributos training          -> x_t\n",
    "    #Clasificacion training      -> y_t\n",
    "    #Atributos de validacion     -> val_x\n",
    "    #Clasificacion de validacion -> val_y\n",
    "    model = Sequential()\n",
    "    #Modelo con 2 capas\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compila el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # Ajusta el modelo\n",
    "    model.fit(x_t, y_t, epochs=i, batch_size=89,verbose=0)\n",
    "    pred = model.predict_classes(val_x)\n",
    "    \n",
    "    #Resultados del clasificador\n",
    "    print(\"N° de epocas: \"+str(i)+\"\\n\")\n",
    "    print(classification_report(pred,val_y))\n",
    "    print(\"Accuracy score: \",accuracy_score(val_y,pred))\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    print(confusion_matrix(val_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 4 capas\n",
      "\n",
      "N° de epocas: 5\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2910\n",
      "           1       0.12      0.84      0.21       107\n",
      "\n",
      "    accuracy                           0.77      3017\n",
      "   macro avg       0.56      0.80      0.54      3017\n",
      "weighted avg       0.96      0.77      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7709645343056016\n",
      "Confusion matrix:\n",
      "\n",
      "[[2236   17]\n",
      " [ 674   90]]\n",
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.86      2814\n",
      "           1       0.17      0.65      0.27       203\n",
      "\n",
      "    accuracy                           0.77      3017\n",
      "   macro avg       0.57      0.71      0.57      3017\n",
      "weighted avg       0.91      0.77      0.82      3017\n",
      "\n",
      "Accuracy score:  0.7669870732515744\n",
      "Confusion matrix:\n",
      "\n",
      "[[2182   71]\n",
      " [ 632  132]]\n",
      "N° de epocas: 15\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2883\n",
      "           1       0.15      0.83      0.25       134\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.80      0.56      3017\n",
      "weighted avg       0.95      0.78      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7759363606231355\n",
      "Confusion matrix:\n",
      "\n",
      "[[2230   23]\n",
      " [ 653  111]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo de 4 capas\\n\")\n",
    "for i in range(5,16,5):\n",
    "    #Atributos training          -> x_t\n",
    "    #Clasificacion training      -> y_t\n",
    "    #Atributos de validacion     -> val_x\n",
    "    #Clasificacion de validacion -> val_y\n",
    "    model = Sequential()\n",
    "    #Modelo con 4 capas\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compila el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # Ajusta el modelo\n",
    "    model.fit(x_t, y_t, epochs=i, batch_size=89, verbose=0)\n",
    "    pred = model.predict_classes(val_x)\n",
    "    \n",
    "    #Resultados del clasificador\n",
    "    print(\"N° de epocas: \"+str(i)+\"\\n\")\n",
    "    print(classification_report(pred,val_y))\n",
    "    print(\"Accuracy score: \",accuracy_score(val_y,pred))\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    print(confusion_matrix(val_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 8 capas\n",
      "N° de epocas: 5\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      0.25      0.40      3016\n",
      "\n",
      "    accuracy                           0.25      3017\n",
      "   macro avg       0.50      0.13      0.20      3017\n",
      "weighted avg       1.00      0.25      0.40      3017\n",
      "\n",
      "Accuracy score:  0.2529002320185615\n",
      "Confusion matrix:\n",
      "\n",
      "[[   0 2253]\n",
      " [   1  763]]\n",
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2873\n",
      "           1       0.15      0.81      0.26       144\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.79      0.56      3017\n",
      "weighted avg       0.95      0.78      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7765992707988068\n",
      "Confusion matrix:\n",
      "\n",
      "[[2226   27]\n",
      " [ 647  117]]\n",
      "N° de epocas: 15\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2890\n",
      "           1       0.15      0.87      0.25       127\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.82      0.56      3017\n",
      "weighted avg       0.96      0.78      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7782565462379848\n",
      "Confusion matrix:\n",
      "\n",
      "[[2237   16]\n",
      " [ 653  111]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo de 8 capas\")\n",
    "for i in range(5,16,5):\n",
    "    #Atributos training          -> x_t\n",
    "    #Clasificacion training      -> y_t\n",
    "    #Atributos de validacion     -> val_x\n",
    "    #Clasificacion de validacion -> val_y\n",
    "    model = Sequential()\n",
    "    #Modelo con 8 capa\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compila el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # Ajusta el modelo\n",
    "    model.fit(x_t, y_t, epochs=i, batch_size=89,verbose=0)\n",
    "    pred = model.predict_classes(val_x)\n",
    "    \n",
    "    #Resultados del clasificador\n",
    "    print(\"N° de epocas: \"+str(i)+\"\\n\")\n",
    "    print(classification_report(pred,val_y))\n",
    "    print(\"Accuracy score: \",accuracy_score(val_y,pred))\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    print(confusion_matrix(val_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 16 capas\n",
      "N° de epocas: 5\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      2869\n",
      "           1       0.15      0.79      0.26       148\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.78      0.56      3017\n",
      "weighted avg       0.95      0.78      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7752734504474643\n",
      "Confusion matrix:\n",
      "\n",
      "[[2222   31]\n",
      " [ 647  117]]\n",
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87      2905\n",
      "           1       0.13      0.90      0.23       112\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.56      0.84      0.55      3017\n",
      "weighted avg       0.96      0.78      0.85      3017\n",
      "\n",
      "Accuracy score:  0.7765992707988068\n",
      "Confusion matrix:\n",
      "\n",
      "[[2242   11]\n",
      " [ 663  101]]\n",
      "N° de epocas: 15\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87      2904\n",
      "           1       0.14      0.93      0.24       113\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.85      0.56      3017\n",
      "weighted avg       0.96      0.78      0.85      3017\n",
      "\n",
      "Accuracy score:  0.7789194564136559\n",
      "Confusion matrix:\n",
      "\n",
      "[[2245    8]\n",
      " [ 659  105]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo de 16 capas\")\n",
    "for i in range(5,16,5):\n",
    "    #Atributos training          -> x_t\n",
    "    #Clasificacion training      -> y_t\n",
    "    #Atributos de validacion     -> val_x\n",
    "    #Clasificacion de validacion -> val_y\n",
    "    model = Sequential()\n",
    "    #Modelo con 16 capas\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compila el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # Ajusta el modelo\n",
    "    model.fit(x_t, y_t, epochs=i, batch_size=89,verbose=0)\n",
    "    pred = model.predict_classes(val_x)\n",
    "    \n",
    "    #Resultados del clasificador\n",
    "    print(\"N° de epocas: \"+str(i)+\"\\n\")\n",
    "    print(classification_report(pred,val_y))\n",
    "    print(\"Accuracy score: \",accuracy_score(val_y,pred))\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    print(confusion_matrix(val_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de 32 capas\n",
      "N° de epocas: 5\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86      2823\n",
      "           1       0.16      0.64      0.26       194\n",
      "\n",
      "    accuracy                           0.77      3017\n",
      "   macro avg       0.57      0.71      0.56      3017\n",
      "weighted avg       0.92      0.77      0.82      3017\n",
      "\n",
      "Accuracy score:  0.7653297978123964\n",
      "Confusion matrix:\n",
      "\n",
      "[[2184   69]\n",
      " [ 639  125]]\n",
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87      2896\n",
      "           1       0.14      0.91      0.25       121\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.84      0.56      3017\n",
      "weighted avg       0.96      0.78      0.85      3017\n",
      "\n",
      "Accuracy score:  0.7795823665893271\n",
      "Confusion matrix:\n",
      "\n",
      "[[2242   11]\n",
      " [ 654  110]]\n",
      "N° de epocas: 15\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87      2868\n",
      "           1       0.16      0.80      0.26       149\n",
      "\n",
      "    accuracy                           0.78      3017\n",
      "   macro avg       0.57      0.79      0.56      3017\n",
      "weighted avg       0.95      0.78      0.84      3017\n",
      "\n",
      "Accuracy score:  0.7762678157109711\n",
      "Confusion matrix:\n",
      "\n",
      "[[2223   30]\n",
      " [ 645  119]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Modelo de 32 capas\")\n",
    "for i in range(5,16,5):\n",
    "    #Atributos training          -> x_t\n",
    "    #Clasificacion training      -> y_t\n",
    "    #Atributos de validacion     -> val_x\n",
    "    #Clasificacion de validacion -> val_y\n",
    "    model = Sequential()\n",
    "    #Modelo con 32 capas\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compila el modelo\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    # Ajusta el modelo\n",
    "    model.fit(x_t, y_t, epochs=i, batch_size=89,verbose=0)\n",
    "    pred = model.predict_classes(val_x)\n",
    "    \n",
    "    #Resultados del clasificador\n",
    "    print(\"N° de epocas: \"+str(i)+\"\\n\")\n",
    "    print(classification_report(pred,val_y))\n",
    "    print(\"Accuracy score: \",accuracy_score(val_y,pred))\n",
    "    print(\"Confusion matrix:\\n\")\n",
    "    print(confusion_matrix(val_y,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de haber realizado el entrenamiento de todas las redes neuronales, se encontró que la que brinda mejores resultados es la red neuronal con 32 capas. Una capa de entrada 31 capas ocultas densas y una capa de salida. Esta red neuronal obtuvo para el conjunto 0 que equivale a las personas que ganan menos de 50k un precision del 100% lo cual indica que el algoritmo puede decir en todas las veces cuando una persona gana menos de 50k. Para el conjunto 1 el cual define a las personas que ganan 50k o más, una precision del 14% lo cual es un valor muy bajo, esto se debe al desbalanceo de las clases. Se puede resaltar el recall del conjunto 1, ya que dio un valor del 91%. Indica que los pocos datos clasificados, quedaron bien clasificados. La taza de aciertos brindada por esta red neuronal es del 77.95%, lo cual es una muy buena taza, ya que demuestra que el algoritmo es capaz de identificar cuando una persona gana menos de 50k, o, 50k o más.\n",
    "\n",
    "## Entrenamiento completo\n",
    "Habiendo encontrado los parámetros que generan el mejor clasificador por redes neuronales, entrenaremos la red con el conjunto de entrenamiento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° de epocas: 10\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86     15060\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75     15060\n",
      "   macro avg       0.50      0.38      0.43     15060\n",
      "weighted avg       1.00      0.75      0.86     15060\n",
      "\n",
      "Accuracy score:  0.7543160690571049\n",
      "Confusion matrix:\n",
      "\n",
      "[[11360     0]\n",
      " [ 3700     0]]\n"
     ]
    }
   ],
   "source": [
    "#Atributos training\n",
    "X = train.loc[:,:13]\n",
    "#Clasificacion training\n",
    "Y = train.loc[:,14]\n",
    "#Atributos de validacion\n",
    "X_t = test.loc[:,:13]\n",
    "#Clasificacion de validacion\n",
    "Y_t = test.loc[:,14]\n",
    "\n",
    "model = Sequential()\n",
    "#Modelo con 32 capas\n",
    "model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(14, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# Compila el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "# Ajusta el modelo\n",
    "model.fit(X, Y, epochs=10, batch_size=89,verbose=0)\n",
    "pred = model.predict_classes(X_t)\n",
    "\n",
    "#Resultados del clasificador\n",
    "print(\"N° de epocas: 10\\n\")\n",
    "print(classification_report(pred,Y_t))\n",
    "print(\"Accuracy score: \",accuracy_score(Y_t,pred))\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(confusion_matrix(Y_t,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al realizar el entrenamiento con el conjunto de datos completo y evaluando el entrenamiento con el conjunto de test, se logró encontrar que el clasificador opto por solo clasificar el conjunto 0. Clasificando el conjunto 0 obtuvo un 100% de precision y de los datos verdaderos el 75% está bien clasificado, lo cual da una certeza la red neuronal es capaz de clasificar a una persona cuando gana menos de 50k. Para el conjunto 1 dio 0% en precision y 0% en recall, lo cual quiere decir que la red neuronal no logra identificar una persona que gana 50k o más. Esto puede verse como si el clasificador evaluara los atributos de una persona y si sus valores no son similares al del conjunto 0, lo clasifica automáticamente como si ganara 50k o más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado de la clasificacion con las tecnicas clasicas de aprendizaje automatico\n",
    "### Arboles de decision\n",
    "<img width=400px src='tree.png'>\n",
    "\n",
    "### Maquina de vectores de soporte\n",
    "<img width=400px src='svc.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "Al comparar los 3 métodos de aprendizaje automático, se puede encontrar que los árboles de decisión brindan el mejor desempeño a la hora de realizar la tarea de clasificación. Debido a que es la que mejor presenta una taza de aciertos con un 84,42%. Además, que el conjunto 1 que se encuentra desbalanceado logra dar, no la mejor, pero si un muy buen precision, \"cuánto es correcto del modelo cuando dice que es correcto\". Con un valor de 0.94 y logra dar un buen recall de todos los métodos, \"de los que realmente se debían clasificar, cuantos se clasificaron\". Con un valor de 0.75.\n",
    "\n",
    "Usando las técnicas clásicas se pudo obtener mejor desempeño a la hora de clasificar a una persona cuando gana menos de 50k o 50k, más específicamente con los árboles de decisión, que con las técnicas avanzadas de aprendizaje automático. Esto no quiere decir que para este problema los árboles de decisión tengan el mejor desempeño, ya que se puede ajustar más los valores de las redes neuronales, pero esto requeriría dedicarle más recursos para lograr ajustar los valores y mejorar el desempeño de los árboles de decisión.\n",
    "\n",
    "Al ser los árboles de decisión una técnica clásica, su implementación y entrenamiento no requieren mucho tiempo, lo que equivale a decir, que, no consumen muchos recursos. Logrando tener alcanzar un gran desempeño.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
